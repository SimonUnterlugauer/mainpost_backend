{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = pd.read_csv(r'C:\\Users\\jakob\\Desktop\\Projektseminar\\Data\\employees.csv', delimiter= '|')\n",
    "absences_df = pd.read_csv(r'C:\\Users\\jakob\\Desktop\\Projektseminar\\Data\\absences.csv', delimiter='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#absences_df = absences_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "absences_df = absences_df.dropna(subset=['start_date', 'end_date'])\n",
    "absences_df = absences_df[absences_df['start_date'].str.contains('2024') == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create an empty list to store the rows of the new DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Step 2: Iterate over the original DataFrame row by row\n",
    "for index, row in absences_df.iterrows():\n",
    "    # Step 3: Generate a list of dates between 'start_date' and 'end_date' (inclusive)\n",
    "    date_range = pd.date_range(start=row['start_date'], end=row['end_date'])\n",
    "    \n",
    "    # Step 4: Duplicate the row for each date in the list and add a new column with the date value\n",
    "    for date in date_range:\n",
    "        new_row = row[['employee_id', 'reason', 'allocation_id']].copy()\n",
    "        new_row['date'] = date\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Step 5: Append the duplicated rows to the empty list\n",
    "\n",
    "# Step 6: Create a new DataFrame using the list of duplicated rows\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#ssave the shit\n",
    "new_df.to_csv(r'C:\\Users\\jakob\\Desktop\\Projektseminar\\Data\\absences_ezrows.csv', index=False)\n",
    "\n",
    "# Display the new DataFrame\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>reason</th>\n",
       "      <th>allocation_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9827</td>\n",
       "      <td>vacation</td>\n",
       "      <td>98044.0</td>\n",
       "      <td>2020-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9827</td>\n",
       "      <td>vacation</td>\n",
       "      <td>98044.0</td>\n",
       "      <td>2020-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9827</td>\n",
       "      <td>vacation</td>\n",
       "      <td>118649.0</td>\n",
       "      <td>2020-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9827</td>\n",
       "      <td>vacation</td>\n",
       "      <td>118649.0</td>\n",
       "      <td>2020-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1366</td>\n",
       "      <td>vacation</td>\n",
       "      <td>2679.0</td>\n",
       "      <td>2020-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301975</th>\n",
       "      <td>9521</td>\n",
       "      <td>vacation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301976</th>\n",
       "      <td>9521</td>\n",
       "      <td>vacation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301977</th>\n",
       "      <td>9521</td>\n",
       "      <td>vacation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301978</th>\n",
       "      <td>9521</td>\n",
       "      <td>vacation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301979</th>\n",
       "      <td>9521</td>\n",
       "      <td>vacation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301980 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        employee_id    reason  allocation_id        date\n",
       "0              9827  vacation        98044.0  2020-01-17\n",
       "1              9827  vacation        98044.0  2020-01-18\n",
       "2              9827  vacation       118649.0  2020-01-17\n",
       "3              9827  vacation       118649.0  2020-01-18\n",
       "4              1366  vacation         2679.0  2020-01-20\n",
       "...             ...       ...            ...         ...\n",
       "301975         9521  vacation            NaN  2022-11-29\n",
       "301976         9521  vacation            NaN  2022-11-30\n",
       "301977         9521  vacation            NaN  2022-12-01\n",
       "301978         9521  vacation            NaN  2022-12-02\n",
       "301979         9521  vacation            NaN  2022-12-03\n",
       "\n",
       "[301980 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv(r'C:\\Users\\jakob\\Desktop\\Projektseminar\\Data\\absences_ezrows.csv', delimiter = ',')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>2024-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>2024-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>2024-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>2024-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>2024-01-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1467 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date\n",
       "0    2020-01-02\n",
       "1    2020-01-03\n",
       "2    2020-01-04\n",
       "3    2020-01-05\n",
       "4    2020-01-06\n",
       "...         ...\n",
       "1462 2024-01-03\n",
       "1463 2024-01-04\n",
       "1464 2024-01-05\n",
       "1465 2024-01-06\n",
       "1466 2024-01-07\n",
       "\n",
       "[1467 rows x 1 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv(r'C:\\Users\\jakob\\Desktop\\Projektseminar\\Data\\absences_ezrows.csv', delimiter = ',')\n",
    "new_df\n",
    "\n",
    "# order new_df by date\n",
    "new_df = new_df.sort_values(by=['date'])\n",
    "\n",
    "min_date = new_df['date'].min()\n",
    "max_date = new_df['date'].max()\n",
    "# Create a DataFrame with the date_range\n",
    "date_range = pd.date_range(start=min_date, end=max_date)\n",
    "all_dates = pd.DataFrame({'Date': date_range})\n",
    "all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 'date' in new_df to datetime\n",
    "# show all unique values in new_df date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns] and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# join the two DataFrames\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m merged_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(all_dates, new_df, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m, left_on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mDate\u001b[39;49m\u001b[39m'\u001b[39;49m, right_on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m#drop Date and employee_id\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#merged_df = merged_df.drop(['Date', 'employee_id'], axis=1)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m merged_df\n",
      "File \u001b[1;32mc:\\Users\\jakob\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 110\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    111\u001b[0m         left,\n\u001b[0;32m    112\u001b[0m         right,\n\u001b[0;32m    113\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    114\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    115\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    116\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    117\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    118\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    119\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    120\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    121\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    122\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\jakob\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:707\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    699\u001b[0m (\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    701\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    702\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m    703\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merge_keys()\n\u001b[0;32m    705\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 707\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_coerce_merge_keys()\n\u001b[0;32m    709\u001b[0m \u001b[39m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[39m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[39m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jakob\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1344\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[39m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \u001b[39melif\u001b[39;00m needs_i8_conversion(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_i8_conversion(rk\u001b[39m.\u001b[39mdtype):\n\u001b[1;32m-> 1344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1345\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_i8_conversion(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m needs_i8_conversion(rk\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m   1346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on datetime64[ns] and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# join the two DataFrames\n",
    "merged_df = pd.merge(all_dates, new_df, how='left', left_on='Date', right_on='date')\n",
    "\n",
    "#drop Date and employee_id\n",
    "#merged_df = merged_df.drop(['Date', 'employee_id'], axis=1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of absences per day in a new column 'count'\n",
    "merged_df['count'] = merged_df.groupby('date')['Date'].transform('count')\n",
    "#drop duplicates of column 'Date'\n",
    "merged_df = merged_df.drop_duplicates(subset=['date'])\n",
    "merged_df['day_of_week'] = merged_df['date'].dt.dayofweek\n",
    "merged_df['month'] = merged_df['date'].dt.month\n",
    "merged_df['season'] = merged_df['date'].dt.quarter\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many NaN values are in the column 'coun'\n",
    "merged_df['count'].isna().sum()\n",
    "# fill NaN with mean of 'count'\n",
    "merged_df['count'] = merged_df['count'].fillna(merged_df['count'].mean())\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new_df as csv\n",
    "merged_df.to_csv(r'C:\\Users\\jakob\\Desktop\\Projektseminar\\Data\\absences_new_for_darts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = new_df[['day_of_week', 'month']]\n",
    "y = new_df['count']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the random forest regression model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = new_df[['day_of_week', 'month']]\n",
    "y = new_df['count']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the random forest regression model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
